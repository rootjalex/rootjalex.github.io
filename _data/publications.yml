scion:
  title: Data Layout Polymorphism for Bounding Volume Hierarchies
  # abstract: "<p>
  #   Bounding volume hierarchies are ubiquitous acceleration structures in graphics,
  #   scientific computing, and data analytics. Their performance depends critically on
  #   data layout choices that affect cache utilization, memory bandwidth, and
  #   vectorization -- increasingly dominant factors in modern computing. Yet, in most
  #   programming systems, these layout choices are hopelessly entangled with the traversal
  #   logic. This entanglement prevents developers from independently optimizing data layouts
  #   and algorithms across different contexts, perpetuating a false dichotomy between
  #   performance and portability. We introduce Scion, a domain-specific language and
  #   compiler for specifying the data layouts of bounding volume hierarchies independent of
  #   tree traversal algorithms. We show that Scion can express a broad spectrum of layout
  #   optimizations used in high performance computing while remaining architecture-agnostic.
  #   We demonstrate empirically that Pareto-optimal layouts (along performance and memory
  #   footprint axes) vary across algorithms, architectures, and workload characteristics.
  #   Through systematic design exploration, we also identify a novel ray tracing layout that
  #   combines optimization techniques from prior work, achieving Pareto-optimality across
  #   diverse architectures and scenes.
  #   </p>"
  venue: "arXiv"
  acronym: Preprint
  # year: 2025
  pdf: https://arxiv.org/abs/2511.15028
  # image:
  # code: https://github.com/rootjalex/burrito-artifact
  # acmdl: https://doi.org/10.1145/3689752
  # month: October
  authors:
    - gyurgyik
    - root
    - kjolstad
  bibtex: "@misc{gyurgyik2025scion,
           <br> title={Data Layout Polymorphism for Bounding Volume Hierarchies},
           <br> author={Christophe Gyurgyik and Alexander J Root and Fredrik Kjolstad},
           <br> year={2025},
           <br> eprint={2511.15028},
           <br> archivePrefix={arXiv},
           <br> primaryClass={cs.PL},
           <br> url={https://arxiv.org/abs/2511.15028},
          <br>}"

bonsai:
  title: Compiling Set Queries into Work-Efficient Tree Traversals
  # abstract: "<p>
  #   Trees can accelerate queries that search or aggregate values over large collections.
  #   They achieve this by storing metadata that enables quick pruning (or inclusion) of
  #   subtrees when predicates on that metadata can prove that none (or all) of the data
  #   in a subtree affect the query result. Existing systems implement this pruning logic
  #   manually for each query predicate and data structure. We generalize and mechanize
  #   this class of optimization. Our method derives conditions for when subtrees can be
  #   pruned (or included wholesale), expressed in terms of the metadata available at each
  #   node. We efficiently generate these conditions using symbolic interval analysis,
  #   extended with new rules to handle geometric predicates (e.g., intersection,
  #   containment). Additionally, our compiler fuses compound queries (e.g., reductions on
  #   filters) into a single tree traversal. These techniques enable the automatic
  #   derivation of generalized single-index and dual-index tree joins that support a wide
  #   class of join predicates beyond standard equality and range predicates. The generated
  #   traversals match the behavior of expert-written code that implements query-specific
  #   traversals, and can asymptotically outperform the linear scans and nested-loop joins
  #   that existing systems fall back to when hand-written cases do not apply.
  #   </p>"
  venue: "arXiv"
  acronym: Preprint
  # year: 2025
  pdf: https://arxiv.org/abs/2511.15000
  # image:
  # code: https://github.com/rootjalex/burrito-artifact
  # acmdl: https://doi.org/10.1145/3689752
  # month: October
  authors:
    - root
    - gyurgyik
    - goel
    - fatahalian
    - jrk
    - adams
    - kjolstad
  bibtex: "@misc{root2025bonsai,
           <br> title={Compiling Set Queries into Work-Efficient Tree Traversals},
           <br> author={Alexander J Root and Christophe Gyurgyik and Purvi Goel and Kayvon Fatahalian and Jonathan Ragan-Kelley and Andrew Adams and Fredrik Kjolstad},
           <br> year={2025},
           <br> eprint={2511.15000},
           <br> archivePrefix={arXiv},
           <br> primaryClass={cs.PL},
           <br> url={https://arxiv.org/abs/2511.15000},
          <br>}"

cgo2026scorch:
  title: Fast Autoscheduling for Sparse ML Frameworks
  abstract: "<p>
    The rapid growth in the size of deep learning models strains the capabilities of dense computation paradigms.
    Leveraging sparse computation has become increasingly popular for training and deploying large-scale models, but existing deep learning frameworks lack extensive support for sparse operations.
    However, existing approaches either require manual scheduling expertise or rely on exhaustive search taking hours to days, which are both incompatible with the interactive development essential to machine learning research.
    We present three algorithmic contributions that enable fast, automatic optimization of sparse tensor computations.
    First, we develop a heuristic-based loop ordering algorithm that avoids asymptotic performance cliffs while compiling in milliseconds rather than hours.
    Second, we introduce a tiling algorithm specialized for mixed sparse-dense computations that achieves cache performance comparable to hand-optimized kernels.
    Third, we present a format inference algorithm that automatically selects appropriate sparse tensor formats for intermediate and output tensors based on operation semantics.
    These algorithms are grounded in the mathematical properties of sparse tensor algebra, making them predictable and robust across diverse workloads.
    We implement these techniques in Scorch, a prototype sparse tensor compiler for PyTorch that demonstrates their practical effectiveness.
    With only minimal code changes, our approach achieves 1.05-5.80x speedups over PyTorch Sparse on end-to-end tasks including graph neural networks, sparse autoencoders, and sparse transformers, while maintaining the interactive compilation speeds essential for ML development.
    </p>"
  venue: "(to appear in) Proceedings of the 24th ACM/IEEE International Symposium on Code Generation and Optimization (CGO)"
  acronym: CGO
  year: 2026
  pdf: https://arxiv.org/pdf/2405.16883
  image:
  code: https://github.com/bobbyyyan/scorch
  # acmdl: https://doi.org/10.1145/3689752
  month: February
  authors:
    - yan
    - root
    - gale
    - broman
    - kjolstad

oopsla2024burrito:
  title: Compilation of Shape Operators on Sparse Arrays
  abstract: "<p>
    We show how to build a compiler for a sparse array language that supports shape operators such as reshaping or
    concatenating arrays, in addition to compute operators. Existing sparse array programming systems implement
    generic shape operators for only some sparse data structures, reduce shape operators on other data structures
    to those, and do not support fusion. Our system compiles sparse array expressions to code that efficiently
    iterates over reshaped views of irregular sparse data structures, without needing to materialize temporary
    storage for intermediates. Our evaluation shows that our approach generates sparse array code competitive with
    popular sparse array libraries: our generated shape operators achieve geometric mean speed-ups of 1.66x-15.3x
    when compared to hand-written kernels in scipy.sparse and 1.67x-651x when compared to generic implementations
    in pydata/sparse. For operators that require data structure conversions in these libraries, our generated code
    achieves geometric mean speed-ups of 7.29x-13.0x when compared to scipy.sparse and 21.3x-511x when compared to
    pydata/sparse. Finally, our evaluation demonstrates that fusing shape and compute operators improves the
    performance of several expressions by geometric mean speed-ups of 1.22x-2.23x.
    </p>"
  venue: "Proceedings of the ACM on Programming Languages, Volume 8, Issue OOPSLA"
  acronym: OOPSLA
  year: 2024
  pdf: /publications/oopsla2024-burrito.pdf
  image:
  code: https://github.com/rootjalex/burrito-artifact
  acmdl: https://doi.org/10.1145/3689752
  month: October
  authors:
    - root
    - yan
    - liu
    - gyurgyik
    - bik
    - kjolstad
  bibtex: "@inproceedings{root2024burrito,
          <br>  author = {Root, Alexander J and Yan, Bobby and Liu, Peiming and Gyurgyik, Christophe and Bik, Aart J.C. and Kjolstad, Fredrik},
          <br>  title = {Compilation of Shape Operators on Sparse Arrays},
          <br>  year = {2024},
          <br>  issue_date = {October 2024},
          <br>  publisher = {Association for Computing Machinery},
          <br>  address = {New York, NY, USA},
          <br>  volume = {8},
          <br>  number = {OOPSLA2},
          <br>  url = {https://doi.org/10.1145/3689752},
          <br>  doi = {10.1145/3689752},
          <br>  abstract = {We show how to build a compiler for a sparse array language that supports shape operators such as reshaping or concatenating arrays, in addition to compute operators. Existing sparse array programming systems implement generic shape operators for only some sparse data structures, reduce shape operators on other data structures to those, and do not support fusion. Our system compiles sparse array expressions to code that efficiently iterates over reshaped views of irregular sparse data structures, without needing to materialize temporary storage for intermediates. Our evaluation shows that our approach generates sparse array code competitive with popular sparse array libraries: our generated shape operators achieve geometric mean speed-ups of 1.66x-15.3x when compared to hand-written kernels in scipy.sparse and 1.67x-651x when compared to generic implementations in pydata/sparse. For operators that require data structure conversions in these libraries, our generated code achieves geometric mean speed-ups of 7.29x-13.0x when compared to scipy.sparse and 21.3x-511x when compared to pydata/sparse. Finally, our evaluation demonstrates that fusing shape and compute operators improves the performance of several expressions by geometric mean speed-ups of 1.22x-2.23x.},
          <br>  journal = {Proc. ACM Program. Lang.},
          <br>  month = oct,
          <br>  articleno = {312},
          <br>  numpages = {27},
          <br>  keywords = {sparse array programming, sparse data structures, sparse iteration theory}
          <br>}"

oopsla2024spconv:
  title: Compiler Support for Sparse Tensor Convolutions
  abstract: "<p>
    This paper extends prior work on sparse tensor algebra compilers to generate asymptotically efficient code for
    tensor expressions with affine subscript expressions. Our technique enables compiler support for a wide range
    of sparse computations, including sparse convolutions and pooling that are widely used in ML and graphics
    applications. We propose an approach that gradually rewrites compound subscript expressions to simple subscript
    expressions with loops that exploit the sparsity pattern of the input sparse tensors. As a result, the time
    complexity of the generated kernels is bounded by the number of stored elements and not by the shape of the
    tensors. Our approach seamlessly integrates into existing frameworks and is compatible with recent advances in
    compilers for sparse computations, including the flexibility to efficiently handle arbitrary combinations of
    different sparse tensor formats. The implementation of our algorithm is open source and upstreamed to the MLIR
    sparse compiler. Experimental results show that our method achieves 19.5x speedup when compared with the
    state-of-the-art compiler-based method at 99.9% sparsity. The generated sparse kernels start to outperform dense
    convolution implementations at about 80% sparsity. 
    </p>"
  venue: "Proceedings of the ACM on Programming Languages, Volume 8, Issue OOPSLA"
  acronym: OOPSLA
  year: 2024
  pdf: /publications/oopsla2024-spconv.pdf
  image:
  code: https://hub.docker.com/r/geticliu/sparse_conv_oopsla
  acmdl: https://doi.org/10.1145/3689721
  month: October
  authors:
    - liu
    - root
    - xu
    - li
    - kjolstad
    - bik
  bibtex: "@inproceedings{liu2024spconv,
          <br>  author = {Liu, Peiming and Root, Alexander J and Xu, Anlun and Li, Yinying and Kjolstad, Fredrik and Bik, Aart J.C.},
          <br>  title = {Compiler Support for Sparse Tensor Convolutions},
          <br>  year = {2024},
          <br>  issue_date = {October 2024},
          <br>  publisher = {Association for Computing Machinery},
          <br>  address = {New York, NY, USA},
          <br>  volume = {8},
          <br>  number = {OOPSLA2},
          <br>  url = {https://doi.org/10.1145/3689721},
          <br>  doi = {10.1145/3689721},
          <br>  abstract = {This paper extends prior work on sparse tensor algebra compilers to generate asymptotically efficient code for tensor expressions with affine subscript expressions. Our technique enables compiler support for a wide range of sparse computations, including sparse convolutions and pooling that are widely used in ML and graphics applications. We propose an approach that gradually rewrites compound subscript expressions to simple subscript expressions with loops that exploit the sparsity pattern of the input sparse tensors. As a result, the time complexity of the generated kernels is bounded by the number of stored elements and not by the shape of the tensors. Our approach seamlessly integrates into existing frameworks and is compatible with recent advances in compilers for sparse computations, including the flexibility to efficiently handle arbitrary combinations of different sparse tensor formats. The implementation of our algorithm is open source and upstreamed to the MLIR sparse compiler. Experimental results show that our method achieves 19.5x speedup when compared with the state-of-the-art compiler-based method at 99.9% sparsity. The generated sparse kernels start to outperform dense convolution implementations at about 80% sparsity.},
          <br>  journal = {Proc. ACM Program. Lang.},
          <br>  month = oct,
          <br>  articleno = {281},
          <br>  numpages = {29},
          <br>  keywords = {code generation, convolution, iteration graphs, merge lattices, performance, sparse data structures, sparse tensor algebra, sparse tensors}
          <br>}"

scorch2024:
  title: "Scorch: A Library for Sparse Deep Learning"
  abstract:
  venue: "Under Submission"
  acronym: arXiv
  year: 2024
  pdf: https://arxiv.org/pdf/2405.16883
  preprint: y
  image:
  code:
  acmdl:
  month: May
  authors:
    - yan
    - root
    - gale
    - broman
    - kjolstad
  bibtex:

asplos2023:
  title: Fast Instruction Selection for Fast Digital Signal Processing
  abstract: "<p>
    Modern vector processors support a wide variety of instructions for fixed-point digital
    signal processing. These instructions sup- port a proliferation of rounding, saturating,
    and type conversion modes, and are often fused combinations of more primitive op- erations.
    While these are common idioms in fixed-point signal processing, it is difficult to use
    these operations in portable code. It is challenging for programmers to write down portable
    integer arithmetic in a C-like language that corresponds exactly to one of these instructions,
    and even more challenging for compilers to recognize when these instructions can be used. Our
    system, Pitch- fork, defines a portable fixed-point intermediate representation, FPIR, that
    captures common idioms in fixed-point code. FPIR can be used directly by programmers experienced
    with fixed-point, or Pitchfork can automatically lift from integer operations into FPIR using a
    term-rewriting system (TRS) composed of verified man- ual and automatically-synthesized rules.
    Pitchfork then lowers from FPIR into target-specific fixed-point instructions using a set of
    target-specific TRSs. We show that this approach improves run- time performance of portably-written
    fixed-point signal processing code in Halide, across a range of benchmarks, by geomean 1.31x on x86
    with AVX2, 1.82x on ARM Neon, and 2.44x on Hexagon HVX compared to a standard LLVM-based compiler flow,
    while maintaining or improving existing compile times.
    </p>"
  venue: "International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)"
  acronym: ASPLOS
  year: 2023
  pdf: /publications/asplos2023-pitchfork.pdf
  image: /assets/pitchfork_arch.png
  code: https://github.com/halide/Halide/tree/rootjalex/trs-codegen
  acmdl: https://doi.org/10.1145/3623278.3624768
  month: March
  authors:
    - root
    - ahmad
    - sharlet
    - adams
    - kamil
    - jrk
  bibtex: "@inproceedings{root2023pitchfork,
          <br>  author = {Root, Alexander J and Ahmad, Maaz Bin Safeer and Sharlet, Dillon and Adams, Andrew and Kamil, Shoaib and Ragan-Kelley, Jonathan},
          <br>  title = {Fast Instruction Selection for Fast Digital Signal Processing},
          <br>  year = {2024},
          <br>  isbn = {9798400703942},
          <br>  publisher = {Association for Computing Machinery},
          <br>  address = {New York, NY, USA},
          <br>  url = {https://doi.org/10.1145/3623278.3624768},
          <br>  doi = {10.1145/3623278.3624768},
          <br>  abstract = {Modern vector processors support a wide variety of instructions for fixed-point digital signal processing. These instructions support a proliferation of rounding, saturating, and type conversion modes, and are often fused combinations of more primitive operations. While these are common idioms in fixed-point signal processing, it is difficult to use these operations in portable code. It is challenging for programmers to write down portable integer arithmetic in a C-like language that corresponds exactly to one of these instructions, and even more challenging for compilers to recognize when these instructions can be used. Our system, Pitchfork, defines a portable fixed-point intermediate representation, FPIR, that captures common idioms in fixed-point code. FPIR can be used directly by programmers experienced with fixed-point, or Pitchfork can automatically lift from integer operations into FPIR using a term-rewriting system (TRS) composed of verified manual and automatically-synthesized rules. Pitchfork then lowers from FPIR into target-specific fixed-point instructions using a set of target-specific TRSs. We show that this approach improves runtime performance of portably-written fixed-point signal processing code in Halide, across a range of benchmarks, by geomean 1.31x on x86 with AVX2, 1.82x on ARM Neon, and 2.44x on Hexagon HVX compared to a standard LLVM-based compiler flow, while maintaining or improving existing compile times.},
          <br>  booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
          <br>  pages = {125-137},
          <br>  numpages = {13},
          <br>  location = {Vancouver, BC, Canada},
          <br>  series = {ASPLOS '23}
          <br>}"

asplos2022:
  title: Vector Instruction Selection for Digital Signal Processors Using Program Synthesis
  abstract: "<p>
    Instruction selection, whereby input code represented in an intermediate representation is
    translated into executable instructions from the target platform, is often the most
    target-dependent component in optimizing compilers. Current approaches include pattern
    matching, which is brittle and tedious to design, or search-based methods, which are
    limited by scalability of the search algorithm. In this paper, we propose a new algorithm
    that first abstracts the target platform instructions into high-level uber-instructions,
    with each uber-instruction unifying multiple concrete instructions from the target platform.
    Program synthesis is used to lift input code sequences into semantically equivalent sequences
    of uber-instructions and then to lower from uber-instructions to machine code. Using 21 real-world
    benchmarks, we show that our synthesis-based instruction selection algorithm can generate instruction
    sequences for a hardware target, with the synthesized code performing up to 2.1x faster as compared
    to code generated by a professionally-developed optimizing compiler for the same platform.
    </p>"
  venue: "International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)"
  acronym: ASPLOS
  year: 2022
  pdf: /publications/asplos2022-rake.pdf
  image: /assets/rake_arch.png
  code: https://github.com/uwplse/rake
  acmdl: https://dl.acm.org/doi/10.1145/3503222.3507714
  month: March
  authors: 
    - ahmad
    - root_mit
    - adams
    - kamil
    - cheung
  bibtex: "@inproceedings{ahmad2022rake,
          <br>  author = {Ahmad, Maaz Bin Safeer and Root, Alexander J and Adams, Andrew and Kamil, Shoaib and Cheung, Alvin},
          <br>  title = {Vector Instruction Selection for Digital Signal Processors Using Program Synthesis},
          <br>  year = {2022},
          <br>  isbn = {9781450392051},
          <br>  publisher = {Association for Computing Machinery},
          <br>  address = {New York, NY, USA},
          <br>  url = {https://doi.org/10.1145/3503222.3507714},
          <br>  doi = {10.1145/3503222.3507714},
          <br>  abstract = {Instruction selection, whereby input code represented in an intermediate representation is translated into executable instructions from the target platform, is often the most target-dependent component in optimizing compilers. Current approaches include pattern matching, which is brittle and tedious to design, or search-based methods, which are limited by scalability of the search algorithm. In this paper, we propose a new algorithm that first abstracts the target platform instructions into high-level uber-instructions, with each uber-instruction unifying multiple concrete instructions from the target platform. Program synthesis is used to lift input code sequences into semantically equivalent sequences of uber-instructions and then to lower from uber-instructions to machine code. Using 21 real-world benchmarks, we show that our synthesis-based instruction selection algorithm can generate instruction sequences for a hardware target, with the synthesized code performing up to 2.1x faster as compared to code generated by a professionally-developed optimizing compiler for the same platform.},
          <br>  booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
          <br>  pages = {1004-1016},
          <br>  numpages = {13},
          <br>  keywords = {Instruction selection, program synthesis, compiler optimizations},
          <br>  location = {Lausanne, Switzerland},
          <br>  series = {ASPLOS '22}
          <br>}"
